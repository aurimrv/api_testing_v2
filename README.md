Repository created for the paper **"Incremental Multi-Model LLM Strategy for Automated White-Box Integration Testing of REST APIs"**, submitted to the Empirical Software Engineering - Advancing Software Engineering with Large Language Models.

---

## Repository Structure

### 1. `EMB_projects/`
- **Description**: Folder containing the EvoMaster Benchmark ([EMB](https://github.com/WebFuzzing/EMB)) projects.  
- **Modifications**:  
  - The configurations have been adapted to enable integration with `Python_scripts/`.

---

### 2. `Generated_tests/`

- **Description**: Repository containing all files generated during the test generation process for the five benchmark projects.  
- **Organization**: Each project (e.g., `countries`) contains three subfolders: `claude`, `evomaster`, and `gpt`.  

- **Subfolders sample (for project `countries`):**

  1. **Claude**  
     - **Location**: `Generated_tests/countries/claude/`  
     - **Content**:  
       - `prompt_templates/`: Templates used for generating tests with the Claude API.  
       - `sonnet35/`: Contains results of executions with Claude Sonnet 3.5.  
         - **Example:** `run01/`  
           - Files generated by the Python script execution.  
           - JaCoCo coverage reports.  
           - Configuration file for running the script.  
           - `exec/`: Folder with the executed tests used for coverage measurement.  

  2. **EvoMaster**  
     - **Location**: `Generated_tests/countries/evomaster/`  
     - **Content**:  
       - `seed01/`: Tests generated by EvoMaster execution and corresponding JaCoCo coverage reports.  

  3. **GPT**  
     - **Location**: `Generated_tests/countries/gpt/`  
     - **Content**:  
       - `prompt_templates/`: Templates used for generating tests with the GPT API.  
       - `gpt4o/`: Results of executions with GPT-4o.  
         - **Example:** `run01/`  
           - Files generated by the Python script execution.  
           - JaCoCo coverage reports.  
           - Configuration file for running the script.  
           - `exec/`: Folder with the executed tests used for coverage measurement.  
       - `gpt35/`: Results of executions with GPT-3.5 Turbo.  
         - **Example:** `run01/`  
           - Files generated by the Python script execution.  
           - JaCoCo coverage reports.  
           - Configuration file for running the script.  
           - `exec/`: Folder with the executed tests used for coverage measurement.

> **Note:** In the case of the **catwatch** project, the `run01` folder inside `sonnet35`, `gpt35`, and `gpt4o` is divided into multiple executions.  
> This is because the script was not able to generate a complete test suite in a single run, as discussed in the *Lessons Learned* section of the paper. 

---

### 3. `Python_scripts/`

- **Description**: Folder containing the Python scripts used to generate and refine test suites with different LLM APIs.  

- **Files**:  
  - **`PythonScript_claude.py`**  
    - Implements the generation and refinement of test suites using the **Claude API**.  
    - Automates the creation of tests based on different prompt versions adapted for Claude models.  

  - **`PythonScript_gpt.py`**  
    - Implements the generation and refinement of test suites using the **OpenAI GPT API**.  
    - Automates the creation of tests based on different prompt versions adapted for GPT models.  

---

## Data Location for Tables and Research Questions

### **RQ1**

This research question analyzed the **evolution of code coverage**, following the sequence: **EvoMaster → GPT-3.5 Turbo → Claude Sonnet 3.5 → GPT-4o** (as described in RQ1 of the paper).  
The collected data shows how coverage improves incrementally from the baseline (EvoMaster) to the successive LLM-based models.

- **EvoMaster**  
  - For each project, the coverage results are located inside the `SID1` folder.  
  - **Example (Countries project):** `Generated_tests/countries/evomaster/seed01/coverage-reports/`  

- **GPT-3.5 Turbo**  
  - Coverage reports for each project are stored inside the `run01` folder of the model.  
  - **Example (Countries project):** `Generated_tests/countries/gpt/gpt35/run01/coverage-reports/`  

- **Claude Sonnet 3.5**  
  - Coverage reports are also located in the `run01` folder of the model.  
  - **Example (Countries project):** `Generated_tests/countries/claude/sonnet35/run01/coverage-reports/`  

- **GPT-4o**  
  - Coverage reports follow the same organization as GPT-3.5 and Claude.  
  - **Example (Countries project):** `Generated_tests/countries/gpt/gpt4o/run01/coverage-reports/`  

---

### **RQ2**
- **Data Related to Table II in the Paper**  
  - **Examples:**  
    - `market-test-set/gpt/v0/coverage_three_runs/`  
    - `market-test-set/gpt/v0/model_combinations/`  
  - These folders contain reports with combinations of test suites generated by `PythonScript.py`.

---